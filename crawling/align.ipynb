{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiyoon/miniconda3/envs/RecLLM/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-03-04 13:09:25.850733: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-03-04 13:09:25.870006: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1741093765.892143  909553 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1741093765.898733  909553 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-03-04 13:09:25.922653: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset, random_split\n",
    "from transformers import BertTokenizerFast, BertForSequenceClassification, AdamW, get_scheduler\n",
    "from tqdm.auto import tqdm\n",
    "import wandb\n",
    "\n",
    "\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from transformers import AutoTokenizer, XLNetModel\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, f1_score\n",
    "import numpy as np\n",
    "from transformers import AutoTokenizer, AutoModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>genre</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tt0000005</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Three men hammer on an anvil and pass a bottle...</td>\n",
       "      <td>Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0000004</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Lost 1892 French short animated film directed ...</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tt0000002</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Lost short film consisting of 300 painted imag...</td>\n",
       "      <td>Animation,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tt0000003</td>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>One night, Arlequin come to see his lover Colo...</td>\n",
       "      <td>Animation,Comedy,Romance</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tt0000001</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Performing on what looks like a small wooden s...</td>\n",
       "      <td>Documentary,Short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207356</th>\n",
       "      <td>390752</td>\n",
       "      <td>tt0407808</td>\n",
       "      <td>Frog and Toad Are Friends</td>\n",
       "      <td>Claymation version of Arnold Lobel's story of ...</td>\n",
       "      <td>Animation,Comedy,Family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207357</th>\n",
       "      <td>390753</td>\n",
       "      <td>tt0407810</td>\n",
       "      <td>From Ardoyne to the √Åras: Inside the McAleese ...</td>\n",
       "      <td>Documentary on the private and public life of ...</td>\n",
       "      <td>Documentary</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207358</th>\n",
       "      <td>390754</td>\n",
       "      <td>tt0407811</td>\n",
       "      <td>Frontstadt</td>\n",
       "      <td>A young filmmaker tries to gain a very persona...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207359</th>\n",
       "      <td>390755</td>\n",
       "      <td>tt0407815</td>\n",
       "      <td>Possible Changes</td>\n",
       "      <td>Two friends, Moon-ho and Jong-kyu, in their mi...</td>\n",
       "      <td>Drama</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207360</th>\n",
       "      <td>390756</td>\n",
       "      <td>tt0407814</td>\n",
       "      <td>Full Grown Men</td>\n",
       "      <td>A man stuck in the reveries of his youth track...</td>\n",
       "      <td>Comedy,Drama</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207361 rows √ó 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         id  \\\n",
       "0                0  tt0000005   \n",
       "1                1  tt0000004   \n",
       "2                2  tt0000002   \n",
       "3                3  tt0000003   \n",
       "4                4  tt0000001   \n",
       "...            ...        ...   \n",
       "207356      390752  tt0407808   \n",
       "207357      390753  tt0407810   \n",
       "207358      390754  tt0407811   \n",
       "207359      390755  tt0407815   \n",
       "207360      390756  tt0407814   \n",
       "\n",
       "                                                    title  \\\n",
       "0                                        Blacksmith Scene   \n",
       "1                                             Un bon bock   \n",
       "2                                  Le clown et ses chiens   \n",
       "3                                            Poor Pierrot   \n",
       "4                                              Carmencita   \n",
       "...                                                   ...   \n",
       "207356                          Frog and Toad Are Friends   \n",
       "207357  From Ardoyne to the √Åras: Inside the McAleese ...   \n",
       "207358                                         Frontstadt   \n",
       "207359                                   Possible Changes   \n",
       "207360                                     Full Grown Men   \n",
       "\n",
       "                                                     desc  \\\n",
       "0       Three men hammer on an anvil and pass a bottle...   \n",
       "1       Lost 1892 French short animated film directed ...   \n",
       "2       Lost short film consisting of 300 painted imag...   \n",
       "3       One night, Arlequin come to see his lover Colo...   \n",
       "4       Performing on what looks like a small wooden s...   \n",
       "...                                                   ...   \n",
       "207356  Claymation version of Arnold Lobel's story of ...   \n",
       "207357  Documentary on the private and public life of ...   \n",
       "207358  A young filmmaker tries to gain a very persona...   \n",
       "207359  Two friends, Moon-ho and Jong-kyu, in their mi...   \n",
       "207360  A man stuck in the reveries of his youth track...   \n",
       "\n",
       "                           genre  \n",
       "0                          Short  \n",
       "1                Animation,Short  \n",
       "2                Animation,Short  \n",
       "3       Animation,Comedy,Romance  \n",
       "4              Documentary,Short  \n",
       "...                          ...  \n",
       "207356   Animation,Comedy,Family  \n",
       "207357               Documentary  \n",
       "207358                     Drama  \n",
       "207359                     Drama  \n",
       "207360              Comedy,Drama  \n",
       "\n",
       "[207361 rows x 5 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('cleaned_imdb_genre.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# primaryTitleÍ≥º descriptionÏùÑ ÌïòÎÇòÏùò ÌÖçÏä§Ìä∏Î°ú Ìï©ÏπòÍ∏∞\n",
    "df['text'] = df['title'].astype(str) + \" \" + df['desc'].astype(str)\n",
    "\n",
    "# genre Ïª¨Îüº Ï†ÑÏ≤òÎ¶¨: ÏâºÌëúÎ°ú Íµ¨Î∂ÑÎêú Î¨∏ÏûêÏó¥ÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò\n",
    "def process_genres(genres_str):\n",
    "    if pd.isna(genres_str):\n",
    "        return []\n",
    "    return [g.strip() for g in genres_str.split(',') if g.strip() != \"\"]\n",
    "\n",
    "df['genre_list'] = df['genre'].apply(process_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ï†ÑÏ≤¥ Ïû•Î•¥: ['Action', 'Adult', 'Adventure', 'Animation', 'Biography', 'Comedy', 'Crime', 'Documentary', 'Drama', 'Family', 'Fantasy', 'Film-Noir', 'Game-Show', 'History', 'Horror', 'Music', 'Musical', 'Mystery', 'News', 'Reality-TV', 'Romance', 'Sci-Fi', 'Short', 'Sport', 'Talk-Show', 'Thriller', 'War', 'Western']\n"
     ]
    }
   ],
   "source": [
    "all_genres = set()\n",
    "for genres in df['genre_list']:\n",
    "    for genre in genres:\n",
    "        all_genres.add(genre)\n",
    "all_genres = sorted(list(all_genres))\n",
    "genre2id = {genre: idx for idx, genre in enumerate(all_genres)}\n",
    "num_labels = len(all_genres)\n",
    "print(\"Ï†ÑÏ≤¥ Ïû•Î•¥:\", all_genres)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Í∞Å ÏÉòÌîåÏóê ÎåÄÌï¥ Î©ÄÌã∞Ìï´ Ïù∏ÏΩîÎî©Îêú Î†àÏù¥Î∏î ÏÉùÏÑ±\n",
    "def encode_labels(genres):\n",
    "    label = [0] * num_labels\n",
    "    for g in genres:\n",
    "        if g in genre2id:\n",
    "            label[genre2id[g]] = 1\n",
    "    return label\n",
    "\n",
    "df['labels'] = df['genre_list'].apply(encode_labels)\n",
    "\n",
    "# Î™®Îç∏ ÌïôÏäµÏóê ÌïÑÏöîÌïú Ïó¥Îßå ÏÑ†ÌÉù\n",
    "df_model = df[['text', 'genre_list', 'labels']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blacksmith Scene Three men hammer on an anvil ...</td>\n",
       "      <td>[Short]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un bon bock Lost 1892 French short animated fi...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le clown et ses chiens Lost short film consist...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poor Pierrot One night, Arlequin come to see h...</td>\n",
       "      <td>[Animation, Comedy, Romance]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carmencita Performing on what looks like a sma...</td>\n",
       "      <td>[Documentary, Short]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207356</th>\n",
       "      <td>Frog and Toad Are Friends Claymation version o...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207357</th>\n",
       "      <td>From Ardoyne to the √Åras: Inside the McAleese ...</td>\n",
       "      <td>[Documentary]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207358</th>\n",
       "      <td>Frontstadt A young filmmaker tries to gain a v...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207359</th>\n",
       "      <td>Possible Changes Two friends, Moon-ho and Jong...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207360</th>\n",
       "      <td>Full Grown Men A man stuck in the reveries of ...</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207361 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                     text  \\\n",
       "0       Blacksmith Scene Three men hammer on an anvil ...   \n",
       "1       Un bon bock Lost 1892 French short animated fi...   \n",
       "2       Le clown et ses chiens Lost short film consist...   \n",
       "3       Poor Pierrot One night, Arlequin come to see h...   \n",
       "4       Carmencita Performing on what looks like a sma...   \n",
       "...                                                   ...   \n",
       "207356  Frog and Toad Are Friends Claymation version o...   \n",
       "207357  From Ardoyne to the √Åras: Inside the McAleese ...   \n",
       "207358  Frontstadt A young filmmaker tries to gain a v...   \n",
       "207359  Possible Changes Two friends, Moon-ho and Jong...   \n",
       "207360  Full Grown Men A man stuck in the reveries of ...   \n",
       "\n",
       "                          genre_list  \\\n",
       "0                            [Short]   \n",
       "1                 [Animation, Short]   \n",
       "2                 [Animation, Short]   \n",
       "3       [Animation, Comedy, Romance]   \n",
       "4               [Documentary, Short]   \n",
       "...                              ...   \n",
       "207356   [Animation, Comedy, Family]   \n",
       "207357                 [Documentary]   \n",
       "207358                       [Drama]   \n",
       "207359                       [Drama]   \n",
       "207360               [Comedy, Drama]   \n",
       "\n",
       "                                                   labels  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "207356  [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "207357  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "207358  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "207359  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "207360  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[207361 rows x 3 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_by_genre(df):\n",
    "    rows = []\n",
    "    for idx, row in df.iterrows():\n",
    "        text = row['text']\n",
    "        genres = row['genre_list']\n",
    "        labels = row['labels']\n",
    "        \n",
    "        for genre in genres:\n",
    "            \n",
    "            # ÏÉà Îç∞Ïù¥ÌÑ∞Î°ú ÏÉùÏÑ± (text, genre, genre_label)\n",
    "            rows.append({\n",
    "                'text': text,\n",
    "                'genre': genre,\n",
    "                'labels': labels\n",
    "            })\n",
    "    \n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "# Ïû•Î•¥Î≥ÑÎ°ú ÌÖçÏä§Ìä∏ÏôÄ ÎùºÎ≤®ÏùÑ Î∂ÑÎ¶¨\n",
    "# genre_df = split_by_genre(df)\n",
    "\n",
    "# genre_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_descriptions = {\n",
    "    \"Action\": \"This movie has thrilling action sequences with intense fight scenes.\",\n",
    "    \"Adult\": \"This film is intended for mature audiences, featuring explicit themes.\",\n",
    "    \"Adventure\": \"This movie takes the audience on an exciting journey full of discovery.\",\n",
    "    \"Animation\": \"This film is beautifully animated with vibrant characters and stunning visuals.\",\n",
    "    \"Biography\": \"This movie tells the true story of a remarkable person's life.\",\n",
    "    \"Comedy\": \"This movie is full of humor and laughter, guaranteed to entertain.\",\n",
    "    \"Crime\": \"This film revolves around criminal activities, investigations, and justice.\",\n",
    "    \"Documentary\": \"This is a factual film that explores real-life events and issues.\",\n",
    "    \"Drama\": \"This movie tells an emotional and heartfelt story with deep character development.\",\n",
    "    \"Family\": \"This movie is suitable for all ages, bringing warmth and joy to families.\",\n",
    "    \"Fantasy\": \"This film takes place in a magical world with fantastical elements and creatures.\",\n",
    "    \"Film-Noir\": \"This movie features a dark and mysterious atmosphere with complex characters.\",\n",
    "    \"Game-Show\": \"This show features competitive games and exciting challenges.\",\n",
    "    \"History\": \"This film brings historical events and figures to life with great detail.\",\n",
    "    \"Horror\": \"This movie contains scary and suspenseful moments that will keep you on edge.\",\n",
    "    \"Music\": \"This film revolves around music, featuring incredible performances and soundtracks.\",\n",
    "    \"Musical\": \"This movie is filled with songs and dance performances that tell a story.\",\n",
    "    \"Mystery\": \"This film keeps the audience guessing with twists and hidden secrets.\",\n",
    "    \"News\": \"This program covers current events and breaking news from around the world.\",\n",
    "    \"Reality-TV\": \"This show follows real people and their lives, providing entertainment and drama.\",\n",
    "    \"Romance\": \"A heartwarming romantic story unfolds in this film, full of love and emotions.\",\n",
    "    \"Sci-Fi\": \"This movie explores futuristic worlds, advanced technology, and space travel.\",\n",
    "    \"Short\": \"This is a short film that tells a compelling story in a brief runtime.\",\n",
    "    \"Sport\": \"This film is centered around sports, athletes, and competitive events.\",\n",
    "    \"Talk-Show\": \"This show features discussions, interviews, and engaging conversations.\",\n",
    "    \"Thriller\": \"This movie is filled with suspense, unexpected twists, and tension.\",\n",
    "    \"War\": \"This film portrays intense battles and the impact of war on people.\",\n",
    "    \"Western\": \"This movie is set in the Old West, featuring cowboys, duels, and frontier life.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "hard_negatives = {\n",
    "    \"Action\": [\"Adventure\", \"Thriller\"],\n",
    "    \"Adult\": [\"Drama\", \"Romance\"],\n",
    "    \"Adventure\": [\"Fantasy\", \"Action\"],\n",
    "    \"Animation\": [\"Family\", \"Fantasy\"],\n",
    "    \"Biography\": [\"History\", \"Drama\"],\n",
    "    \"Comedy\": [\"Family\", \"Musical\"],\n",
    "    \"Crime\": [\"Thriller\", \"Drama\"],\n",
    "    \"Documentary\": [\"History\", \"News\"],\n",
    "    \"Drama\": [\"Romance\", \"Biography\"],\n",
    "    \"Family\": [\"Animation\", \"Comedy\"],\n",
    "    \"Fantasy\": [\"Sci-Fi\", \"Adventure\"],\n",
    "    \"Film-Noir\": [\"Mystery\", \"Thriller\"],\n",
    "    \"Game-Show\": [\"Reality-TV\", \"Talk-Show\"],\n",
    "    \"History\": [\"Biography\", \"Documentary\"],\n",
    "    \"Horror\": [\"Thriller\", \"Mystery\"],\n",
    "    \"Music\": [\"Musical\", \"Drama\"],\n",
    "    \"Musical\": [\"Music\", \"Comedy\"],\n",
    "    \"Mystery\": [\"Thriller\", \"Crime\"],\n",
    "    \"News\": [\"Documentary\", \"Talk-Show\"],\n",
    "    \"Reality-TV\": [\"Game-Show\", \"Talk-Show\"],\n",
    "    \"Romance\": [\"Drama\", \"Comedy\"],\n",
    "    \"Sci-Fi\": [\"Fantasy\", \"Action\"],\n",
    "    \"Short\": [\"Documentary\", \"Animation\"],\n",
    "    \"Sport\": [\"Drama\", \"Action\"],\n",
    "    \"Talk-Show\": [\"Reality-TV\", \"News\"],\n",
    "    \"Thriller\": [\"Horror\", \"Mystery\"],\n",
    "    \"War\": [\"History\", \"Drama\"],\n",
    "    \"Western\": [\"Adventure\", \"Action\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>genre_list</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Three men hammer on an anvil and pass a bottle...</td>\n",
       "      <td>[Short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Lost 1892 French short animated film directed ...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Lost short film consisting of 300 painted imag...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>One night, Arlequin come to see his lover Colo...</td>\n",
       "      <td>[Animation, Comedy, Romance]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Performing on what looks like a small wooden s...</td>\n",
       "      <td>[Documentary, Short]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207356</th>\n",
       "      <td>Frog and Toad Are Friends</td>\n",
       "      <td>Claymation version of Arnold Lobel's story of ...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207357</th>\n",
       "      <td>From Ardoyne to the √Åras: Inside the McAleese ...</td>\n",
       "      <td>Documentary on the private and public life of ...</td>\n",
       "      <td>[Documentary]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207358</th>\n",
       "      <td>Frontstadt</td>\n",
       "      <td>A young filmmaker tries to gain a very persona...</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207359</th>\n",
       "      <td>Possible Changes</td>\n",
       "      <td>Two friends, Moon-ho and Jong-kyu, in their mi...</td>\n",
       "      <td>[Drama]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207360</th>\n",
       "      <td>Full Grown Men</td>\n",
       "      <td>A man stuck in the reveries of his youth track...</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207361 rows √ó 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                    title  \\\n",
       "0                                        Blacksmith Scene   \n",
       "1                                             Un bon bock   \n",
       "2                                  Le clown et ses chiens   \n",
       "3                                            Poor Pierrot   \n",
       "4                                              Carmencita   \n",
       "...                                                   ...   \n",
       "207356                          Frog and Toad Are Friends   \n",
       "207357  From Ardoyne to the √Åras: Inside the McAleese ...   \n",
       "207358                                         Frontstadt   \n",
       "207359                                   Possible Changes   \n",
       "207360                                     Full Grown Men   \n",
       "\n",
       "                                                     desc  \\\n",
       "0       Three men hammer on an anvil and pass a bottle...   \n",
       "1       Lost 1892 French short animated film directed ...   \n",
       "2       Lost short film consisting of 300 painted imag...   \n",
       "3       One night, Arlequin come to see his lover Colo...   \n",
       "4       Performing on what looks like a small wooden s...   \n",
       "...                                                   ...   \n",
       "207356  Claymation version of Arnold Lobel's story of ...   \n",
       "207357  Documentary on the private and public life of ...   \n",
       "207358  A young filmmaker tries to gain a very persona...   \n",
       "207359  Two friends, Moon-ho and Jong-kyu, in their mi...   \n",
       "207360  A man stuck in the reveries of his youth track...   \n",
       "\n",
       "                          genre_list  \n",
       "0                            [Short]  \n",
       "1                 [Animation, Short]  \n",
       "2                 [Animation, Short]  \n",
       "3       [Animation, Comedy, Romance]  \n",
       "4               [Documentary, Short]  \n",
       "...                              ...  \n",
       "207356   [Animation, Comedy, Family]  \n",
       "207357                 [Documentary]  \n",
       "207358                       [Drama]  \n",
       "207359                       [Drama]  \n",
       "207360               [Comedy, Drama]  \n",
       "\n",
       "[207361 rows x 3 columns]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_df = df[['title', 'desc', 'genre_list']]\n",
    "model_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>title</th>\n",
       "      <th>desc</th>\n",
       "      <th>genre</th>\n",
       "      <th>text</th>\n",
       "      <th>genre_list</th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>tt0000005</td>\n",
       "      <td>Blacksmith Scene</td>\n",
       "      <td>Three men hammer on an anvil and pass a bottle...</td>\n",
       "      <td>Short</td>\n",
       "      <td>Blacksmith Scene Three men hammer on an anvil ...</td>\n",
       "      <td>[Short]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>tt0000004</td>\n",
       "      <td>Un bon bock</td>\n",
       "      <td>Lost 1892 French short animated film directed ...</td>\n",
       "      <td>Animation,Short</td>\n",
       "      <td>Un bon bock Lost 1892 French short animated fi...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>tt0000002</td>\n",
       "      <td>Le clown et ses chiens</td>\n",
       "      <td>Lost short film consisting of 300 painted imag...</td>\n",
       "      <td>Animation,Short</td>\n",
       "      <td>Le clown et ses chiens Lost short film consist...</td>\n",
       "      <td>[Animation, Short]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>tt0000003</td>\n",
       "      <td>Poor Pierrot</td>\n",
       "      <td>One night, Arlequin come to see his lover Colo...</td>\n",
       "      <td>Animation,Comedy,Romance</td>\n",
       "      <td>Poor Pierrot One night, Arlequin come to see h...</td>\n",
       "      <td>[Animation, Comedy, Romance]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>tt0000001</td>\n",
       "      <td>Carmencita</td>\n",
       "      <td>Performing on what looks like a small wooden s...</td>\n",
       "      <td>Documentary,Short</td>\n",
       "      <td>Carmencita Performing on what looks like a sma...</td>\n",
       "      <td>[Documentary, Short]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207356</th>\n",
       "      <td>390752</td>\n",
       "      <td>tt0407808</td>\n",
       "      <td>Frog and Toad Are Friends</td>\n",
       "      <td>Claymation version of Arnold Lobel's story of ...</td>\n",
       "      <td>Animation,Comedy,Family</td>\n",
       "      <td>Frog and Toad Are Friends Claymation version o...</td>\n",
       "      <td>[Animation, Comedy, Family]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207357</th>\n",
       "      <td>390753</td>\n",
       "      <td>tt0407810</td>\n",
       "      <td>From Ardoyne to the √Åras: Inside the McAleese ...</td>\n",
       "      <td>Documentary on the private and public life of ...</td>\n",
       "      <td>Documentary</td>\n",
       "      <td>From Ardoyne to the √Åras: Inside the McAleese ...</td>\n",
       "      <td>[Documentary]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207358</th>\n",
       "      <td>390754</td>\n",
       "      <td>tt0407811</td>\n",
       "      <td>Frontstadt</td>\n",
       "      <td>A young filmmaker tries to gain a very persona...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Frontstadt A young filmmaker tries to gain a v...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207359</th>\n",
       "      <td>390755</td>\n",
       "      <td>tt0407815</td>\n",
       "      <td>Possible Changes</td>\n",
       "      <td>Two friends, Moon-ho and Jong-kyu, in their mi...</td>\n",
       "      <td>Drama</td>\n",
       "      <td>Possible Changes Two friends, Moon-ho and Jong...</td>\n",
       "      <td>[Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>207360</th>\n",
       "      <td>390756</td>\n",
       "      <td>tt0407814</td>\n",
       "      <td>Full Grown Men</td>\n",
       "      <td>A man stuck in the reveries of his youth track...</td>\n",
       "      <td>Comedy,Drama</td>\n",
       "      <td>Full Grown Men A man stuck in the reveries of ...</td>\n",
       "      <td>[Comedy, Drama]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>207361 rows √ó 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Unnamed: 0         id  \\\n",
       "0                0  tt0000005   \n",
       "1                1  tt0000004   \n",
       "2                2  tt0000002   \n",
       "3                3  tt0000003   \n",
       "4                4  tt0000001   \n",
       "...            ...        ...   \n",
       "207356      390752  tt0407808   \n",
       "207357      390753  tt0407810   \n",
       "207358      390754  tt0407811   \n",
       "207359      390755  tt0407815   \n",
       "207360      390756  tt0407814   \n",
       "\n",
       "                                                    title  \\\n",
       "0                                        Blacksmith Scene   \n",
       "1                                             Un bon bock   \n",
       "2                                  Le clown et ses chiens   \n",
       "3                                            Poor Pierrot   \n",
       "4                                              Carmencita   \n",
       "...                                                   ...   \n",
       "207356                          Frog and Toad Are Friends   \n",
       "207357  From Ardoyne to the √Åras: Inside the McAleese ...   \n",
       "207358                                         Frontstadt   \n",
       "207359                                   Possible Changes   \n",
       "207360                                     Full Grown Men   \n",
       "\n",
       "                                                     desc  \\\n",
       "0       Three men hammer on an anvil and pass a bottle...   \n",
       "1       Lost 1892 French short animated film directed ...   \n",
       "2       Lost short film consisting of 300 painted imag...   \n",
       "3       One night, Arlequin come to see his lover Colo...   \n",
       "4       Performing on what looks like a small wooden s...   \n",
       "...                                                   ...   \n",
       "207356  Claymation version of Arnold Lobel's story of ...   \n",
       "207357  Documentary on the private and public life of ...   \n",
       "207358  A young filmmaker tries to gain a very persona...   \n",
       "207359  Two friends, Moon-ho and Jong-kyu, in their mi...   \n",
       "207360  A man stuck in the reveries of his youth track...   \n",
       "\n",
       "                           genre  \\\n",
       "0                          Short   \n",
       "1                Animation,Short   \n",
       "2                Animation,Short   \n",
       "3       Animation,Comedy,Romance   \n",
       "4              Documentary,Short   \n",
       "...                          ...   \n",
       "207356   Animation,Comedy,Family   \n",
       "207357               Documentary   \n",
       "207358                     Drama   \n",
       "207359                     Drama   \n",
       "207360              Comedy,Drama   \n",
       "\n",
       "                                                     text  \\\n",
       "0       Blacksmith Scene Three men hammer on an anvil ...   \n",
       "1       Un bon bock Lost 1892 French short animated fi...   \n",
       "2       Le clown et ses chiens Lost short film consist...   \n",
       "3       Poor Pierrot One night, Arlequin come to see h...   \n",
       "4       Carmencita Performing on what looks like a sma...   \n",
       "...                                                   ...   \n",
       "207356  Frog and Toad Are Friends Claymation version o...   \n",
       "207357  From Ardoyne to the √Åras: Inside the McAleese ...   \n",
       "207358  Frontstadt A young filmmaker tries to gain a v...   \n",
       "207359  Possible Changes Two friends, Moon-ho and Jong...   \n",
       "207360  Full Grown Men A man stuck in the reveries of ...   \n",
       "\n",
       "                          genre_list  \\\n",
       "0                            [Short]   \n",
       "1                 [Animation, Short]   \n",
       "2                 [Animation, Short]   \n",
       "3       [Animation, Comedy, Romance]   \n",
       "4               [Documentary, Short]   \n",
       "...                              ...   \n",
       "207356   [Animation, Comedy, Family]   \n",
       "207357                 [Documentary]   \n",
       "207358                       [Drama]   \n",
       "207359                       [Drama]   \n",
       "207360               [Comedy, Drama]   \n",
       "\n",
       "                                                   labels  \n",
       "0       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2       [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3       [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "4       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "...                                                   ...  \n",
       "207356  [0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, ...  \n",
       "207357  [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "207358  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "207359  [0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "207360  [0, 0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...  \n",
       "\n",
       "[207361 rows x 8 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü¶• Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
      "ü¶• Unsloth Zoo will now patch everything to make training faster!\n",
      "==((====))==  Unsloth 2025.2.15: Fast Llama patching. Transformers: 4.47.1.\n",
      "   \\\\   /|    GPU: NVIDIA RTX A4000. Max memory: 15.731 GB. Platform: Linux.\n",
      "O^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 8.6. CUDA Toolkit: 12.4. Triton: 3.2.0\n",
      "\\        /    Bfloat16 = TRUE. FA [Xformers = 0.0.29.post3. FA2 = False]\n",
      " \"-____-\"     Free Apache license: http://github.com/unslothai/unsloth\n",
      "Unsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:04<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModel\n",
    "from unsloth import FastLanguageModel\n",
    "\n",
    "# Î™®Îç∏ Î°úÎìú (8-bit Ï†ÅÏö©)\n",
    "MODEL_NAME = \"unsloth/phi-4-unsloth-bnb-4bit\"\n",
    "load_in_4bit = True\n",
    "max_seq_length = 1024\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = MODEL_NAME,\n",
    "    max_seq_length = max_seq_length,\n",
    "    load_in_4bit = load_in_4bit,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unsloth 2025.2.15 patched 40 layers with 40 QKV layers, 40 O layers and 40 MLP layers.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 65,536,000 || all params: 14,725,043,200 || trainable%: 0.4451\n"
     ]
    }
   ],
   "source": [
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = 16, # Choose any number > 0 ! Suggested 8, 16, 32, 64, 128\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", # self-attention Î†àÏù¥Ïñ¥\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\",], # MLP Î†àÏù¥Ïñ¥ (Ïû•Î•¥ Ï†ïÎ≥¥ Ïù∏ÏΩîÎî© Í∞ïÌôî)\n",
    "    lora_alpha = 32,\n",
    "    lora_dropout = 0, # Supports any, but = 0 is optimized\n",
    "    bias = \"none\",    # Supports any, but = \"none\" is optimized\n",
    "    # [NEW] \"unsloth\" uses 30% less VRAM, fits 2x larger batch sizes!\n",
    "    use_gradient_checkpointing = \"unsloth\", # True or \"unsloth\" for very long context\n",
    "    random_state = 3407,\n",
    "    use_rslora = False,  # We support rank stabilized LoRA\n",
    "    loftq_config = None, # And LoftQ\n",
    ")\n",
    "\n",
    "model.to(device)\n",
    "model.print_trainable_parameters()  # ÌïôÏäµ Í∞ÄÎä•Ìïú ÌååÎùºÎØ∏ÌÑ∞Îßå ÌôïÏù∏"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        param.data = param.data.to(model.dtype)  # Î™®Îç∏ dtype(BFloat16)Í≥º ÎßûÏ∂îÍ∏∞\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight torch.Size([1280, 16])\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight torch.Size([5120, 16])\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight torch.Size([16, 5120])\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight torch.Size([17920, 16])\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight torch.Size([16, 17920])\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight torch.Size([5120, 16])\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if \"lora\" in name:\n",
    "        print(name, param.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Trainable Parameters: 65536000\n"
     ]
    }
   ],
   "source": [
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f\"Total Trainable Parameters: {trainable_params}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def get_filtered_negative_samples(genre_list, all_genres, num_neg_samples=3):\n",
    "    \n",
    "    negative_candidates = list(set(all_genres) - set(genre_list))\n",
    "\n",
    "    hard_neg_candidates = []\n",
    "    for genre in genre_list:\n",
    "        if genre in hard_negatives:\n",
    "            hard_neg_candidates.extend(hard_negatives[genre])\n",
    "\n",
    "    # Hard Negative ÌõÑÎ≥¥ Ï§ëÏóêÏÑú Ïã§Ï†ú Negative ÌõÑÎ≥¥ÏôÄ Í≤πÏπòÎäî Í≤ÉÎßå ÏÑ†ÌÉù\n",
    "    hard_neg_candidates = list(set(hard_neg_candidates) & set(negative_candidates))\n",
    "\n",
    "    # ÏµúÏ¢Ö Negative ÏÉòÌîåÎßÅ (Hard Negative + Ï∂îÍ∞Ä Negative)\n",
    "    if len(hard_neg_candidates) < num_neg_samples:\n",
    "        # Hard NegativeÍ∞Ä Î∂ÄÏ°±ÌïòÎ©¥ ÏùºÎ∞ò NegativeÏóêÏÑú Ï∂îÍ∞Ä\n",
    "        additional_negatives = list(set(negative_candidates) - set(hard_neg_candidates))\n",
    "        sampled_additional_negatives = random.sample(additional_negatives, num_neg_samples - len(hard_neg_candidates))\n",
    "        final_neg_samples = hard_neg_candidates + sampled_additional_negatives\n",
    "    else:\n",
    "        # Hard NegativeÍ∞Ä Ï∂©Î∂ÑÌïòÎ©¥ Í±∞Í∏∞ÏÑúÎßå ÏÉòÌîåÎßÅ\n",
    "        final_neg_samples = random.sample(hard_neg_candidates, num_neg_samples)\n",
    "\n",
    "    # Ïû•Î•¥ ÏÑ§Î™Ö ÌÖçÏä§Ìä∏ Î≥ÄÌôò\n",
    "    neg_texts = [label_descriptions[neg] for neg in final_neg_samples]\n",
    "\n",
    "    return neg_texts\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# Îç∞Ïù¥ÌÑ∞ÏÖã ÌÅ¥ÎûòÏä§ Ï†ïÏùò\n",
    "class ContrastiveDataset(Dataset):\n",
    "    def __init__(self, df, tokenizer, all_genres=None, max_length=128, mode=\"train\"):\n",
    "        self.df = df\n",
    "        self.tokenizer = tokenizer\n",
    "        self.all_genres = all_genres\n",
    "        self.max_length = max_length\n",
    "        self.mode = mode\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.df.iloc[idx]\n",
    "\n",
    "        title = row[\"title\"]\n",
    "        description = row[\"desc\"]\n",
    "        genre_list = row[\"genre_list\"]\n",
    "\n",
    "        if self.mode == \"train\":\n",
    "\n",
    "            neg_texts = get_filtered_negative_samples(genre_list, all_genres)\n",
    "\n",
    "            story_prompt = f\"Movie Title: {title}, Story: {description}\"\n",
    "            label_prompt_pos = [\"Label: \" + label_descriptions[pos] for pos in genre_list]\n",
    "            label_prompt_neg = [\"Label: \" + neg for neg in neg_texts]  # Ïó¨Îü¨ Í∞úÏùò Negative ÏÉòÌîå\n",
    "\n",
    "            text_enc = self.tokenizer(story_prompt, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "            pos_enc = self.tokenizer(label_prompt_pos, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "            neg_enc = self.tokenizer(label_prompt_neg, truncation=True, padding='max_length', max_length=self.max_length, return_tensors=\"pt\")\n",
    "\n",
    "            return {\n",
    "                \"text_input_ids\": text_enc[\"input_ids\"].squeeze(0),\n",
    "                \"text_attention_mask\": text_enc[\"attention_mask\"].squeeze(0),\n",
    "                \"positive_input_ids\": pos_enc[\"input_ids\"],\n",
    "                \"positive_attention_mask\": pos_enc[\"attention_mask\"],\n",
    "                \"negative_input_ids\": neg_enc[\"input_ids\"],\n",
    "                \"negative_attention_mask\": neg_enc[\"attention_mask\"]\n",
    "            }\n",
    "        \n",
    "        else:\n",
    "            return {\n",
    "                \"title\": title,\n",
    "                \"description\": description,\n",
    "                \"answer\": genre_list\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "train_df, val_df = train_test_split(model_df, test_size=0.2, random_state=42)\n",
    "\n",
    "train_dataset = ContrastiveDataset(train_df, tokenizer, all_genres=all_genres)\n",
    "val_dataset = ContrastiveDataset(val_df, tokenizer, all_genres=all_genres)\n",
    "test_dataset = ContrastiveDataset(val_df, tokenizer, mode=\"test\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "def train_collate_fn(batch):\n",
    "    \"\"\"Î∞∞Ïπò ÎÇ¥ `positive_input_ids`Ïùò ÌÅ¨Í∏∞Î•º ÎßûÏ∂îÎäî Ìï®Ïàò\"\"\"\n",
    "\n",
    "    text_input_ids = torch.stack([b[\"text_input_ids\"] for b in batch])\n",
    "    text_attention_mask = torch.stack([b[\"text_attention_mask\"] for b in batch])\n",
    "\n",
    "    # üîπ Positive ÏÉòÌîå Ìå®Îî© Ï†ÅÏö© (Í∞ÄÏû• ÌÅ∞ `num_positives` Í∏∞Ï§Ä)\n",
    "    max_pos_samples = max([b[\"positive_input_ids\"].shape[0] for b in batch])  # Î∞∞Ïπò ÎÇ¥ Í∞ÄÏû• Í∏¥ Í∏çÏ†ï ÏÉòÌîå Í∞úÏàò Ï∞æÍ∏∞\n",
    "    pos_input_ids = [torch.cat([b[\"positive_input_ids\"], torch.zeros(max_pos_samples - b[\"positive_input_ids\"].shape[0], b[\"positive_input_ids\"].shape[1])]) if b[\"positive_input_ids\"].shape[0] < max_pos_samples else b[\"positive_input_ids\"] for b in batch]\n",
    "    pos_attention_mask = [torch.cat([b[\"positive_attention_mask\"], torch.zeros(max_pos_samples - b[\"positive_attention_mask\"].shape[0], b[\"positive_attention_mask\"].shape[1])]) if b[\"positive_attention_mask\"].shape[0] < max_pos_samples else b[\"positive_attention_mask\"] for b in batch]\n",
    "\n",
    "    pos_input_ids = torch.stack(pos_input_ids)\n",
    "    pos_attention_mask = torch.stack(pos_attention_mask)\n",
    "\n",
    "    # üîπ Negative ÏÉòÌîå (3Í∞úÎ°ú Í≥†Ï†ï)\n",
    "    neg_input_ids = torch.stack([b[\"negative_input_ids\"] for b in batch])\n",
    "    neg_attention_mask = torch.stack([b[\"negative_attention_mask\"] for b in batch])\n",
    "\n",
    "    return {\n",
    "        \"text_input_ids\": text_input_ids,\n",
    "        \"text_attention_mask\": text_attention_mask,\n",
    "        \"positive_input_ids\": pos_input_ids.to(torch.long),\n",
    "        \"positive_attention_mask\": pos_attention_mask,\n",
    "        \"negative_input_ids\": neg_input_ids.to(torch.long),\n",
    "        \"negative_attention_mask\": neg_attention_mask\n",
    "    }\n",
    "\n",
    "\n",
    "def collate_fn(batch):\n",
    "    \"\"\"\n",
    "    Test Îç∞Ïù¥ÌÑ∞ÏóêÏÑú Î∞∞Ïπò ÌÅ¨Í∏∞ Î∂àÏùºÏπò Î¨∏Ï†ú Ìï¥Í≤∞\n",
    "    - `title`, `description`, `answer`Î•º Î¶¨Ïä§Ìä∏Î°ú Ïú†ÏßÄÌïòÏó¨ DataLoaderÍ∞Ä Ï≤òÎ¶¨ Í∞ÄÎä•ÌïòÎèÑÎ°ù Ìï®\n",
    "    \"\"\"\n",
    "    titles = [item[\"title\"] for item in batch]\n",
    "    descriptions = [item[\"description\"] for item in batch]\n",
    "    answers = [item[\"answer\"] for item in batch]  # Î¶¨Ïä§Ìä∏ ÌòïÌÉú Ïú†ÏßÄ\n",
    "\n",
    "    return {\n",
    "        \"title\": titles,\n",
    "        \"description\": descriptions,\n",
    "        \"answer\": answers\n",
    "    }\n",
    "\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=4, shuffle=True, collate_fn=train_collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=4, shuffle=False, collate_fn=train_collate_fn)  # Í≤ÄÏ¶ù Îç∞Ïù¥ÌÑ∞Îäî shuffle X\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=4, shuffle=False, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.bfloat16"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype = model.dtype\n",
    "dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# InfoNCE ÏÜêÏã§ Ìï®Ïàò Ï†ïÏùò\n",
    "def info_nce_loss(query, positives, negatives, temperature=0.07):\n",
    "    query = F.normalize(query, p=2, dim=-1)\n",
    "    positives = F.normalize(positives, p=2, dim=-1)\n",
    "    negatives = F.normalize(negatives, p=2, dim=-1)\n",
    "\n",
    "    # Í∏çÏ†ï ÏÉòÌîåÎì§Îì§Í≥ºÏùò Ïú†ÏÇ¨ÎèÑ\n",
    "    pos_sim = torch.exp(torch.matmul(query.unsqueeze(1), positives.permute(0, 2, 1)).squeeze(1) / temperature)  # (batch_size, num_pos_samples)\n",
    "\n",
    "    # Î∂ÄÏ†ï ÏÉòÌîåÎì§Í≥ºÏùò Ïú†ÏÇ¨ÎèÑ (Ïó¨Îü¨ Í∞úÏùò Î∂ÄÏ†ï ÏÉòÌîå Ìè¨Ìï®)\n",
    "    neg_sim = torch.exp(torch.matmul(query.unsqueeze(1), negatives.permute(0, 2, 1)).squeeze(1) / temperature)  # (batch_size, num_neg_samples)\n",
    "\n",
    "    # InfoNCE ÏÜêÏã§ Í≥ÑÏÇ∞ (Î™®Îì† Î∂ÄÏ†ï ÏÉòÌîåÏùÑ Í≥†Î†§)\n",
    "    pos_sim_sum = torch.sum(pos_sim, dim=-1)  # (batch_size)\n",
    "    neg_sim_sum = torch.sum(neg_sim, dim=-1)  # (batch_size)\n",
    "\n",
    "    # üöÄ Loss Í≥ÑÏÇ∞\n",
    "    loss = -torch.log(pos_sim_sum / (pos_sim_sum + neg_sim_sum + 1e-8))\n",
    "\n",
    "    return loss.mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model.parameters()), lr=2e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    param_group['params'] = [p.to(model.dtype) for p in param_group['params']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n",
      "Optimizer Param dtype: torch.bfloat16, Model dtype: torch.bfloat16\n"
     ]
    }
   ],
   "source": [
    "for param_group in optimizer.param_groups:\n",
    "    for param in param_group['params']:\n",
    "        print(f\"Optimizer Param dtype: {param.dtype}, Model dtype: {model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.0.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.1.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.2.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.3.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.4.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.5.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.6.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.7.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.8.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.9.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.10.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.11.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.12.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.13.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.14.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.15.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.16.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.17.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.18.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.19.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.20.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.21.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.22.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.23.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.24.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.25.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.26.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.27.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.28.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.29.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.30.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.31.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.32.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.33.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.34.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.35.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.36.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.37.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.38.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.q_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.k_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.v_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.self_attn.o_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.gate_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.up_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_A.default.weight: torch.bfloat16, requires_grad=True\n",
      "base_model.model.model.layers.39.mlp.down_proj.lora_B.default.weight: torch.bfloat16, requires_grad=True\n"
     ]
    }
   ],
   "source": [
    "for name, param in model.named_parameters():\n",
    "    if param.requires_grad:\n",
    "        print(f\"{name}: {param.dtype}, requires_grad={param.requires_grad}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3:  87%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñã | 36054/41472 [69:14:40<10:16:24,  6.83s/it, loss=0.977]"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "# üöÄ Í≤ÄÏ¶ù(Validation) Ìï®Ïàò Ï†ïÏùò\n",
    "def validation_step(model, val_dataloader):\n",
    "    model.eval()\n",
    "    total_val_loss = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(val_dataloader, avg_train_loss, desc=\"Validating\", leave=False):\n",
    "\n",
    "            query_emb = model(batch[\"text_input_ids\"], batch[\"text_attention_mask\"]).last_hidden_state[:, 0]\n",
    "\n",
    "            batch_size, num_positives, seq_len = batch[\"positive_input_ids\"].shape  \n",
    "            pos_input_ids = batch[\"positive_input_ids\"].reshape(batch_size * num_positives, seq_len)\n",
    "            pos_attention_mask = batch[\"positive_attention_mask\"].reshape(batch_size * num_positives, seq_len)\n",
    "\n",
    "            pos_output = model(input_ids=pos_input_ids, attention_mask=pos_attention_mask, output_hidden_states=True)\n",
    "            pos_emb = pos_output.hidden_states[-1][:, 0].reshape(batch_size, num_positives, -1)  # ÏõêÎûò Î∞∞Ïπò ÌòïÌÉúÎ°ú Î≥µÍµ¨\n",
    "\n",
    "            batch_size, num_negatives, seq_len = batch[\"negative_input_ids\"].shape  # ÌòÑÏû¨ Negative Samples ÌÅ¨Í∏∞ ÌôïÏù∏\n",
    "            #`negative_input_ids`Î•º `batch_size * num_negatives`Î°ú Reshape\n",
    "            neg_input_ids = batch[\"negative_input_ids\"].reshape(batch_size * num_negatives, seq_len)\n",
    "            neg_attention_mask = batch[\"negative_attention_mask\"].reshape(batch_size * num_negatives, seq_len)\n",
    "\n",
    "            # Negative Embedding ÏñªÍ∏∞\n",
    "            neg_output = model(input_ids=neg_input_ids, attention_mask=neg_attention_mask, output_hidden_states=True)\n",
    "            neg_emb = neg_output.hidden_states[-1][:, 0].reshape(batch_size, num_negatives, -1)  # ÏõêÎûò Î∞∞Ïπò ÌòïÌÉúÎ°ú Î≥µÍµ¨\n",
    "\n",
    "\n",
    "            # InfoNCE ÏÜêÏã§ Í≥ÑÏÇ∞\n",
    "            loss = info_nce_loss(query_emb, pos_emb, neg_emb)\n",
    "            total_val_loss += loss.item()\n",
    "\n",
    "            # üî• ÏòàÏ∏°Í∞í ÏÉùÏÑ± (queryÏôÄ posÏùò ÏΩîÏÇ¨Ïù∏ Ïú†ÏÇ¨ÎèÑÎ•º Í∏∞Î∞òÏúºÎ°ú ÏòàÏ∏°)\n",
    "            # query_emb = F.normalize(query_emb, p=2, dim=-1)\n",
    "            # pos_emb = F.normalize(pos_emb, p=2, dim=-1)\n",
    "            # similarity = torch.matmul(query_emb, pos_emb.T).cpu().numpy()\n",
    "            # preds = (similarity > 0.5).astype(int)  # Threshold = 0.5\n",
    "            # labels = torch.ones_like(similarity)\n",
    "\n",
    "            # all_preds.extend(preds.flatten())\n",
    "            # all_labels.extend(labels.flatten())\n",
    "\n",
    "    avg_val_loss = total_val_loss / len(val_dataloader)\n",
    "    # accuracy = accuracy_score(all_labels, all_preds)\n",
    "    # f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "    wandb.log({\"Train Loss\": avg_train_loss, \"Val Loss\": avg_val_loss})\n",
    "\n",
    "    return avg_val_loss\n",
    "\n",
    "# ÌïôÏäµ Î£®ÌîÑ (tqdm Ï†ÅÏö© + Validation Ï∂îÍ∞Ä)\n",
    "num_epochs = 3\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    train_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "\n",
    "    for batch in train_bar:\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        batch[\"text_input_ids\"] = batch[\"text_input_ids\"].to(device)\n",
    "        batch[\"text_attention_mask\"] = batch[\"text_attention_mask\"].to(device)\n",
    "        batch[\"positive_input_ids\"] = batch[\"positive_input_ids\"].to(device)\n",
    "        batch[\"positive_attention_mask\"] = batch[\"positive_attention_mask\"].to(device)\n",
    "        batch[\"negative_input_ids\"] = batch[\"negative_input_ids\"].to(device)\n",
    "        batch[\"negative_attention_mask\"] = batch[\"negative_attention_mask\"].to(device)\n",
    "\n",
    "        query_emb = model(batch[\"text_input_ids\"], batch[\"text_attention_mask\"], output_hidden_states=True).hidden_states[-1][:, 0]\n",
    "\n",
    "        batch_size, num_positives, seq_len = batch[\"positive_input_ids\"].shape  \n",
    "        pos_input_ids = batch[\"positive_input_ids\"].view(batch_size * num_positives, seq_len)\n",
    "        pos_attention_mask = batch[\"positive_attention_mask\"].view(batch_size * num_positives, seq_len)\n",
    "\n",
    "        pos_output = model(input_ids=pos_input_ids, attention_mask=pos_attention_mask, output_hidden_states=True)\n",
    "        pos_emb = pos_output.hidden_states[-1][:, 0].view(batch_size, num_positives, -1)  # ÏõêÎûò Î∞∞Ïπò ÌòïÌÉúÎ°ú Î≥µÍµ¨\n",
    "\n",
    "\n",
    "        batch_size, num_negatives, seq_len = batch[\"negative_input_ids\"].shape  # ÌòÑÏû¨ Negative Samples ÌÅ¨Í∏∞ ÌôïÏù∏\n",
    "        #`negative_input_ids`Î•º `batch_size * num_negatives`Î°ú Reshape\n",
    "        neg_input_ids = batch[\"negative_input_ids\"].view(batch_size * num_negatives, seq_len)\n",
    "        neg_attention_mask = batch[\"negative_attention_mask\"].view(batch_size * num_negatives, seq_len)\n",
    "\n",
    "        # Negative Embedding ÏñªÍ∏∞\n",
    "        neg_output = model(input_ids=neg_input_ids, attention_mask=neg_attention_mask, output_hidden_states=True)\n",
    "        neg_emb = neg_output.hidden_states[-1][:, 0].view(batch_size, num_negatives, -1)  # ÏõêÎûò Î∞∞Ïπò ÌòïÌÉúÎ°ú Î≥µÍµ¨\n",
    "\n",
    "        # InfoNCE ÏÜêÏã§ Í≥ÑÏÇ∞\n",
    "        loss = info_nce_loss(query_emb, pos_emb, neg_emb)\n",
    "        loss = loss.to(model.dtype)\n",
    "\n",
    "        # print(f\"Loss dtype: {loss.dtype}, Model dtype: {model.dtype}\")\n",
    "        # for param in model.parameters():\n",
    "        #     if param.grad is not None:\n",
    "        #         print(f\"Grad dtype: {param.grad.dtype}, Model dtype: {model.dtype}\")\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        train_bar.set_postfix(loss=loss.item())\n",
    "\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "\n",
    "    # üöÄ Í≤ÄÏ¶ù Ïã§Ìñâ\n",
    "    avg_val_loss = validation_step(model, val_dataloader, avg_train_loss)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train Loss = {avg_train_loss:.4f}, Val Loss = {avg_val_loss:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# üöÄ Ï†ÄÏû•Ìï† ÎîîÎ†âÌÜ†Î¶¨ ÏÑ§Ï†ï\n",
    "save_directory = \"fine_tuned_phi4_lora\"\n",
    "os.makedirs(save_directory, exist_ok=True)\n",
    "\n",
    "# ‚úÖ LoRA Adapter Ï†ÄÏû•\n",
    "model.save_pretrained(save_directory)  # LoRA Adapter Ï†ÄÏû•\n",
    "tokenizer.save_pretrained(save_directory)  # ÌÜ†ÌÅ¨ÎÇòÏù¥Ï†Ä Ï†ÄÏû•\n",
    "\n",
    "print(f\"‚úÖ Model and LoRA adapter saved at {save_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import transformers\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score\n",
    "\n",
    "# üîπ Hugging Face Text Generation Pipeline ÏÑ§Ï†ï\n",
    "pipeline = transformers.pipeline(\n",
    "    \"text-generation\",\n",
    "    model=\"unsloth/phi-4-unsloth-bnb-4bit\",\n",
    "    model_kwargs={\"torch_dtype\": \"auto\"},\n",
    "    device_map=\"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n",
      "Loading checkpoint shards: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 3/3 [00:07<00:00,  2.50s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n",
      "Device set to use cuda:0\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def evaluate_llm_generation(test_dataloader, all_genres, output_file=\"llm_predictions.txt\"):\n",
    "    \"\"\"\n",
    "    - `unsloth/phi-4-unsloth-bnb-4bit` Î™®Îç∏ÏùÑ ÏÇ¨Ïö©ÌïòÏó¨ Ïû•Î•¥Î•º ÏòàÏ∏°ÌïòÍ≥† ÌèâÍ∞Ä\n",
    "    - `.txt` ÌååÏùºÏóê `[Prompt] [LLM Predictions] [Answer]` ÌòïÏãùÏúºÎ°ú Ï†ÄÏû•\n",
    "    - Precision, Recall, F1-score Í≥ÑÏÇ∞ ÌõÑ ÌååÏùºÏóê Ï∂îÍ∞Ä\n",
    "    \"\"\"\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"LLM Movie Genre Prediction Results\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\\n\")\n",
    "\n",
    "    for batch in test_dataloader:\n",
    "        batch = {k: v for k, v in batch.items()}  # ÌÉÄÏù¥ÌãÄ, ÏÑ§Î™Ö, Ï†ïÎãµ Ìè¨Ìï®\n",
    "\n",
    "        # üöÄ ÌîÑÎ°¨ÌîÑÌä∏ Î©îÏãúÏßÄ ÏÉùÏÑ± (ÎåÄÌôîÌòï Î©îÏãúÏßÄ Ìè¨Îß∑ Ï†ÅÏö©)\n",
    "        prompts = [\n",
    "            [\n",
    "                {\"role\": \"system\", \"content\": f\"You are an AI movie genre classifier. Your task is to assign the most appropriate genres to a movie. Follow these rules: 1. Choose ONLY from the given genres: {', '.join(all_genres)}. 2. Output ONLY the predicted genres as a comma-separated list. 3. Do NOT repeat or copy the full genre list. 4. Do NOT add explanations or extra text.\"},\n",
    "                {\"role\": \"user\", \"content\": f\"Movie Title: {title}, Story: {desc}, Predicted Genres:\"}\n",
    "            ]\n",
    "            for title, desc in zip(batch[\"title\"], batch[\"description\"])\n",
    "        ]\n",
    "        \n",
    "\n",
    "        # üöÄ Î™®Îç∏ ÏòàÏ∏° ÏàòÌñâ (Î∞∞Ïπò Îã®ÏúÑÎ°ú Ï≤òÎ¶¨)\n",
    "        outputs = [pipeline(prompt, max_new_tokens=15)[0][\"generated_text\"][-1] for prompt in prompts]\n",
    "\n",
    "        # üöÄ LLMÏù¥ ÏÉùÏÑ±Ìïú Ïû•Î•¥ ÌïÑÌÑ∞ÎßÅ (Ïò¨Î∞îÎ•∏ Ïû•Î•¥Îßå Ìè¨Ìï®)\n",
    "        filtered_preds = []\n",
    "        for pred in outputs:\n",
    "            pred_text = pred['content']\n",
    "            pred_lst = [genre.strip() for genre in pred_text.split(',')]\n",
    "            # pred_genres = [genre for genre in all_genres if genre in pred_lst]  # Ï†ïÌï¥ÏßÑ Ïû•Î•¥ Î™©Î°ùÏóê Ìè¨Ìï®Îêú Í≤ÉÎßå ÏÑ†ÌÉù\n",
    "            filtered_preds.append(pred_lst)\n",
    "\n",
    "        # üöÄ Ïã§Ï†ú Ï†ïÎãµ Í∞ÄÏ†∏Ïò§Í∏∞\n",
    "        actual_labels = batch[\"answer\"]\n",
    "\n",
    "        # üöÄ ÌååÏùºÏóê Í∏∞Î°ù (ÏßÄÏ†ïÎêú ÌòïÏãù Ï†ÅÏö©)\n",
    "        with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "            for prompt, pred, actual in zip(prompts, filtered_preds, actual_labels):\n",
    "                f.write(f\"[Prompt]\\n{prompt}\\n\\n\")\n",
    "                f.write(f\"[LLM Predictions]\\n{', '.join(pred)}\\n\\n\")\n",
    "                f.write(f\"[Answer]\\n{', '.join(actual)}\\n\")\n",
    "                f.write(\"-\" * 100 + \"\\n\\n\")\n",
    "\n",
    "        # üöÄ Î™®Îç∏Ïùò ÏòàÏ∏°Í∞íÏùÑ Î¶¨Ïä§Ìä∏Î°ú Î≥ÄÌôò\n",
    "        pred_vectors = [[1 if genre in pred else 0 for genre in all_genres] for pred in filtered_preds]\n",
    "        label_vectors = [[1 if genre in actual else 0 for genre in all_genres] for actual in actual_labels]\n",
    "\n",
    "        all_preds.extend(pred_vectors)\n",
    "        all_labels.extend(label_vectors)\n",
    "\n",
    "    # üöÄ Precision, Recall, F1-score Í≥ÑÏÇ∞\n",
    "    precision = precision_score(all_labels, all_preds, average=\"macro\")\n",
    "    recall = recall_score(all_labels, all_preds, average=\"macro\")\n",
    "    f1 = f1_score(all_labels, all_preds, average=\"macro\")\n",
    "\n",
    "    print(f\"LLM Generation - Precision: {precision:.4f}, Recall: {recall:.4f}, F1-score: {f1:.4f}\")\n",
    "\n",
    "    # üöÄ ÌèâÍ∞Ä Í≤∞Í≥º ÌååÏùºÏóê Ï†ÄÏû•\n",
    "    with open(output_file, \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\" + \"=\" * 100 + \"\\n\")\n",
    "        f.write(f\"Final Evaluation Metrics:\\n\")\n",
    "        f.write(f\"Precision: {precision:.4f}\\n\")\n",
    "        f.write(f\"Recall: {recall:.4f}\\n\")\n",
    "        f.write(f\"F1-score: {f1:.4f}\\n\")\n",
    "        f.write(\"=\" * 100 + \"\\n\")\n",
    "\n",
    "    return precision, recall, f1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM Generation - Precision: 0.4044, Recall: 0.6624, F1-score: 0.4574\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.4044343079616163, 0.6624283520022937, 0.4574190720193066)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_llm_generation(test_dataloader, all_genres)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "RecLLM",
   "language": "python",
   "name": "recllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
