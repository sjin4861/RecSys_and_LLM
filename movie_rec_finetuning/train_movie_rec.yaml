model_name_or_path: meta-llama/Llama-3.2-1B-Instruct
dataset_path: /Users/jun/Desktop/25_1/RecSys_and_LLM/recsys_and_llm/ml/redial_dataset/train_data_llamafactory.jsonl
val_dataset_path: /Users/jun/Desktop/25_1/RecSys_and_LLM/recsys_and_llm/ml/redial_dataset/test_data_llamafactory.jsonl
template: llama3

output_dir: /Users/jun/Desktop/25_1/RecSys_and_LLM/movie_rec_finetuning/output

# 훈련 하이퍼파라미터
num_train_epochs: 3
per_device_train_batch_size: 4
per_device_eval_batch_size: 4
gradient_accumulation_steps: 2
learning_rate: 5e-5
max_grad_norm: 1.0
weight_decay: 0.01
warmup_ratio: 0.03
eval_strategy: steps
eval_steps: 500
save_strategy: steps
save_steps: 500
save_total_limit: 3
logging_steps: 100
max_seq_length: 1024
dataloader_num_workers: 4
gradient_checkpointing: true
lr_scheduler_type: cosine

# LoRA 하이퍼파라미터
lora_rank: 8
lora_alpha: 32
lora_dropout: 0.05
target_modules: [q_proj, k_proj, v_proj, o_proj, gate_proj, up_proj, down_proj]

# fp16/bf16
bf16: auto

# RoPE 스케일링으로 컨텍스트 확장
rope_scaling: linear
rope_scaling_factor: 2.0

# 추가 옵션
flash_attn: auto
use_fast_tokenizer: false
seed: 42