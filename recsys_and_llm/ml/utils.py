import html
import os
import re
from collections import Counter, defaultdict
from datetime import datetime

import numpy as np
from pytz import timezone


def create_dir(directory):
    if not os.path.exists(directory):
        os.makedirs(directory)


# ex. target_word: .csv / in target_path find 123.csv file
def find_filepath(target_path, target_word):
    file_paths = []
    for file in os.listdir(target_path):
        if os.path.isfile(os.path.join(target_path, file)):
            if target_word in file:
                file_paths.append(target_path + file)

    return file_paths


def get_missing(title_id_dict):
    exist_items = [k for k, v in title_id_dict.items() if v != "No Title"]

    return np.setdiff1d(
        np.arange(1, max(exist_items) + 1, dtype=np.int32),
        exist_items,
        assume_unique=True,
    ).tolist()


def find_cold(user_collection, max_len):
    data = {}

    # db data -> dict
    for user_data in user_collection.find():
        user_id = str(user_data["_id"])  # _idë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        itemnums = [
            item["itemnum"] for item in user_data.get("items", [])
        ]  # itemnum ë¦¬ìŠ¤íŠ¸ ì¶”ì¶œ

        # ìµœì‹  max_len ê°œìˆ˜ë§Œ ìœ ì§€ (íŒ¨ë”© í¬í•¨)
        seq = [0] * max_len
        recent_items = itemnums[
            -max_len:
        ]  # ë§ˆì§€ë§‰ ì•„ì´í…œì„ ì œì™¸í•œ ìµœì‹  max_lenê°œë§Œ ìœ ì§€
        seq[-len(recent_items) :] = recent_items  # íŒ¨ë”© í¬í•¨í•˜ì—¬ ìš°ì¸¡ ì •ë ¬

        data[user_id] = seq  # ê²°ê³¼ ì €ì¥

    item_interactions = []
    for user, items in data.items():
        item_interactions.extend(items)

    # Step 2: Count occurrences of each item
    item_counts = Counter(item_interactions)

    # Step 3: Sort items by interaction count
    sorted_items = sorted(
        item_counts.items(), key=lambda x: x[1], reverse=True
    )  # (item, count)

    # Step 4: Calculate thresholds for warm and cold items
    total_items = len(sorted_items)
    cold_threshold = int(total_items * 0.35)  # Bottom 35%

    # Get cold items (bottom 35% of interactions)
    cold_items = [item for item, count in sorted_items[-cold_threshold:]]

    return cold_items


def get_text_name_dict(item_collection):
    text_name_dict = {"title": {}, "description": {}}

    for item in item_collection.find():
        item_id = int(item["_id"])  # _idë¥¼ ë¬¸ìì—´ë¡œ ë³€í™˜
        title = item.get("title", "No Title")  # ê¸°ë³¸ê°’ ì„¤ì •
        description = item.get("description", ["No Description"])  # ê¸°ë³¸ê°’ ì„¤ì •

        # ë¦¬ìŠ¤íŠ¸ íƒ€ì…ì´ë©´ ë¬¸ìì—´ë¡œ ë³€í™˜
        if isinstance(description, list):
            description = " ".join(description) if description else "No Description"

        text_name_dict["title"][item_id] = title
        text_name_dict["description"][item_id] = description

    return text_name_dict


def make_candidate_for_LLM(model, itemnum, log_emb, log_seq, args):
    seq = np.zeros([args.maxlen], dtype=np.int32)
    idx = args.maxlen - 1
    for i in reversed(log_seq):
        seq[idx] = i
        idx -= 1
        if idx == -1:
            break
    rated = set(seq)
    rated.add(0)

    item_idx = []
    for t in range(1, itemnum + 1):
        if t in rated:
            continue
        item_idx.append(t)

    # predictions = -model.predict(*[np.array(l) for l in [[-1], [seq], item_idx]])
    predictions = -model.predict(
        np.array([-1]),  # user_id placeholder
        np.array([seq]),  # sequence
        np.array(item_idx),  # candidate items
        log_emb=log_emb,  # ìœ ì € ì„ë² ë”©
    )
    predictions = predictions[0]  # - for 1st argsort DESC

    # Top-K ì•„ì´í…œ ì„ íƒ (ê°€ì¥ ë†’ì€ ì ìˆ˜ ê¸°ì¤€)
    top_k = 20
    top_k_indices = predictions.argsort()[
        :top_k
    ]  # ì ìˆ˜ê°€ ë†’ì€ Top-K ì•„ì´í…œì˜ ì¸ë±ìŠ¤ ì„ íƒ

    # ì‹¤ì œ ì•„ì´í…œ ë²ˆí˜¸ë¡œ ë³€í™˜
    top_k_items = [
        item_idx[idx] for idx in top_k_indices
    ]  # ì¸ë±ìŠ¤ë¥¼ ì•„ì´í…œ ë²ˆí˜¸ë¡œ ë§¤í•‘

    return top_k_items


def seq_preprocess(maxlen, data):
    seq = np.zeros([maxlen], dtype=np.int32)
    idx = maxlen - 1
    for i in reversed(data):
        seq[idx] = i
        idx -= 1
        if idx == -1:
            break

    return seq


def clean_text(text):
    text = str(text)  # ë¬¸ìì—´ ë³€í™˜
    text = html.unescape(text)  # ğŸ”¹ HTML ì—”í‹°í‹° ë³€í™˜ (&amp; â†’ & ë“±)

    # ğŸ”¹ íŠ¹ìˆ˜ ê³µë°± ë° ì œì–´ ë¬¸ì ì œê±° (ìœ ë‹ˆì½”ë“œ í¬í•¨)
    text = text.replace("\xa0", " ").replace("\t", " ").replace("\n", " ")

    # ğŸ”¹ ì•ë’¤ ê³µë°± ë° í°ë”°ì˜´í‘œ ì œê±°
    text = text.strip().strip('"').strip()

    # ğŸ”¹ ì¤‘ë³µ ê³µë°± ì œê±° (ëª¨ë“  ì¢…ë¥˜ì˜ ê³µë°± í¬í•¨)
    text = re.sub(r"\s+", " ", text, flags=re.UNICODE)
    return text


def calculate_genre_distribution(item_collection, all_genres):
    """
    ì „ì²´ ì˜í™” ë°ì´í„°ì—ì„œ ì¥ë¥´ ë¶„í¬ë¥¼ ê³„ì‚°í•˜ì—¬ ì „ì—­ ì¥ë¥´ ë¹ˆë„ìˆ˜ ë°˜í™˜
    """
    genre_counts = Counter()
    total_genre_mappings = 0

    for movie in item_collection.find({}, {"predicted_genre": 1}):
        if "predicted_genre" in movie:
            genre_counts.update(movie["predicted_genre"])
            total_genre_mappings += len(movie["predicted_genre"])

    # ì¥ë¥´ë³„ ë¶„í¬ í™•ë¥  ê³„ì‚° (P(genre))
    genre_distribution = {
        genre: genre_counts[genre] / total_genre_mappings for genre in all_genres
    }
    return genre_distribution
